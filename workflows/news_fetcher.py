#!/usr/bin/env python3
"""
è´¢ç»æ–°é—»é‡‡é›†å¼•æ“ v1.0
ä¸ºæŠ•èµ„Agentçš„10ä¸ªSkillæä¾›å®æ—¶æ–°é—»æ•°æ®ï¼ˆè¿‡å»24å°æ—¶ï¼‰

æ•°æ®æºï¼šGoogle News RSSï¼ˆè‹±æ–‡+ä¸­æ–‡åŒè¯­é‡‡é›†ï¼‰
æ¶æ„ï¼šæ¯ä¸ªSkillå®šä¹‰ä¸“å±æœç´¢å…³é”®è¯ â†’ å¹¶è¡Œé‡‡é›† â†’ å»é‡ â†’ æ—¶æ•ˆæ€§è¿‡æ»¤ â†’ è¿”å›ç»“æ„åŒ–æ–°é—»åˆ—è¡¨
"""

import os
import time
import json
import hashlib
import requests
import feedparser
from datetime import datetime, timedelta, timezone
from urllib.parse import quote
from typing import Dict, List, Optional
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ–°é—»æ•°æ®ç»“æ„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class NewsItem:
    """å•æ¡æ–°é—»"""
    title: str
    source: str
    published: str          # å‘å¸ƒæ—¶é—´å­—ç¬¦ä¸²
    published_dt: Optional[datetime] = None  # å‘å¸ƒæ—¶é—´datetimeï¼ˆå¯èƒ½ä¸ºNoneï¼‰
    url: str = ""
    language: str = "en"    # en / zh
    skill_tags: list = field(default_factory=list)  # å…³è”çš„Skillç¼–å·
    relevance: float = 1.0  # ç›¸å…³æ€§åˆ†æ•°

    def to_dict(self):
        return {
            'title': self.title,
            'source': self.source,
            'published': self.published,
            'url': self.url,
            'language': self.language,
            'skill_tags': self.skill_tags,
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Skillæœç´¢å…³é”®è¯å®šä¹‰ï¼ˆè‹±æ–‡+ä¸­æ–‡ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# æ¯ä¸ªSkillçš„æœç´¢æŸ¥è¯¢ï¼ˆç²¾ç®€ç‰ˆï¼šè‹±æ–‡2æ¡+ä¸­æ–‡1æ¡ï¼Œwhen:1dé™åˆ¶è¿‡å»24å°æ—¶ï¼‰
SKILL_QUERIES = {
    1: {
        'name': 'å…¬å¸ä¼°å€¼ä¸è´¨é‡è¯„çº§',
        'en': [
            'NVDA AAPL MSFT TSLA META AMZN stock earnings valuation',
            'tech stock mega cap earnings P/E growth outlook',
        ],
        'zh': [
            'ç¾è‚¡ ç§‘æŠ€è‚¡ è´¢æŠ¥ ä¼°å€¼ è‹±ä¼Ÿè¾¾ è‹¹æœ',
        ],
    },
    2: {
        'name': 'åŠ å¯†è´§å¸å‘¨æœŸä¸æŠ„åº•',
        'en': [
            'bitcoin ethereum crypto market price crash rally',
            'cryptocurrency SEC regulation ETF stablecoin',
        ],
        'zh': [
            'æ¯”ç‰¹å¸ ä»¥å¤ªåŠ åŠ å¯†è´§å¸ æš´è·Œ æš´æ¶¨',
        ],
    },
    3: {
        'name': 'å…¨çƒå¸‚åœºæƒ…ç»ªç›‘æ§',
        'en': [
            'market sentiment fear greed VIX crash correction',
            'investor sentiment put call ratio stock market rally',
        ],
        'zh': [
            'ç¾è‚¡ å¸‚åœºæƒ…ç»ª ææ…Œ è´ªå©ª å¤§è·Œ å¤§æ¶¨',
        ],
    },
    4: {
        'name': 'å®è§‚æµåŠ¨æ€§ä¸å¤®è¡Œç›‘æ§',
        'en': [
            'federal reserve interest rate monetary policy inflation',
            'ECB BOJ central bank rate decision QE QT liquidity',
        ],
        'zh': [
            'ç¾è”å‚¨ é™æ¯ åŠ æ¯ è´§å¸æ”¿ç­– é€šèƒ€',
        ],
    },
    5: {
        'name': 'å…¨çƒå¸‚åœºè”åŠ¨ä¸èµ„é‡‘æµå‘',
        'en': [
            'global stock market Asia Europe capital flow rotation',
            'emerging markets fund flow S&P 500 Nasdaq sector',
        ],
        'zh': [
            'å…¨çƒè‚¡å¸‚ èµ„é‡‘æµå‘ æ¿å—è½®åŠ¨ æ–°å…´å¸‚åœº',
        ],
    },
    6: {
        'name': 'ä¿¡è´·å¸‚åœºä¸ç§å‹Ÿä¿¡ç”¨ç›‘æ§',
        'en': [
            'private credit default high yield bond CLO leveraged loan',
            'Blue Owl Ares KKR TPG credit fund SaaS software debt',
        ],
        'zh': [
            'ç§å‹Ÿä¿¡è´· é«˜æ”¶ç›Šå€º è¿çº¦ æ æ†è´·æ¬¾ ä¿¡ç”¨é£é™©',
        ],
    },
    7: {
        'name': 'è´µé‡‘å±ä¸å¤§å®—å•†å“å‘¨æœŸ',
        'en': [
            'gold silver copper oil price commodity',
            'OPEC crude WTI Brent supply demand inflation commodity',
        ],
        'zh': [
            'é»„é‡‘ ç™½é“¶ åŸæ²¹ å¤§å®—å•†å“ é“œä»·',
        ],
    },
    8: {
        'name': 'æ”¶ç›Šç‡æ›²çº¿ä¸åˆ©ç‡åˆ†æ',
        'en': [
            'treasury yield curve inversion 10 year bond rate',
            'mortgage rate spread housing interest rate',
        ],
        'zh': [
            'ç¾å€º æ”¶ç›Šç‡ åˆ©ç‡ æ›²çº¿å€’æŒ‚ å›½å€º',
        ],
    },
    9: {
        'name': 'æ³¢åŠ¨ç‡å¾®è§‚ç»“æ„',
        'en': [
            'VIX volatility spike options expiration 0DTE gamma',
            'implied volatility skew term structure market',
        ],
        'zh': [
            'VIX æ³¢åŠ¨ç‡ æœŸæƒ å¸‚åœºæ³¢åŠ¨',
        ],
    },
    10: {
        'name': 'æ¸¯è‚¡ä¸Aè‚¡ä¸“é¡¹åˆ†æ',
        'en': [
            'Hong Kong stock Hang Seng China market A-share',
            'China policy stimulus southbound northbound tech property',
        ],
        'zh': [
            'æ¸¯è‚¡ Aè‚¡ æ’æŒ‡ å—å‘èµ„é‡‘ ä¸­æ¦‚è‚¡ è…¾è®¯ é˜¿é‡Œ',
        ],
    },
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Google News RSS é‡‡é›†å™¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    'Accept': 'application/rss+xml, application/xml, text/xml',
}

# ç¼“å­˜ç›®å½•
CACHE_DIR = os.path.join(os.path.dirname(__file__), "investment_agent_data", "news_cache")


def _parse_pub_date(entry) -> Optional[datetime]:
    """è§£æRSSæ¡ç›®çš„å‘å¸ƒæ—¶é—´"""
    try:
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            from calendar import timegm
            ts = timegm(entry.published_parsed)
            return datetime.fromtimestamp(ts, tz=timezone.utc)
    except Exception:
        pass

    # å°è¯•è§£æpublishedå­—ç¬¦ä¸²
    pub_str = entry.get('published', '')
    for fmt in ['%a, %d %b %Y %H:%M:%S %Z', '%a, %d %b %Y %H:%M:%S %z',
                '%Y-%m-%dT%H:%M:%S%z', '%Y-%m-%d %H:%M:%S']:
        try:
            return datetime.strptime(pub_str, fmt).replace(tzinfo=timezone.utc)
        except (ValueError, TypeError):
            continue
    return None


def _extract_source(title: str) -> tuple:
    """ä»Google Newsæ ‡é¢˜ä¸­æå–æ¥æºï¼ˆæ ¼å¼ï¼š'æ ‡é¢˜ - æ¥æº'ï¼‰"""
    if ' - ' in title:
        parts = title.rsplit(' - ', 1)
        return parts[0].strip(), parts[1].strip()
    return title, 'Unknown'


def fetch_google_news(query: str, lang: str = 'en', max_results: int = 8) -> List[NewsItem]:
    """
    ä»Google News RSSè·å–æ–°é—»
    Args:
        query: æœç´¢å…³é”®è¯
        lang: è¯­è¨€ ('en' æˆ– 'zh')
        max_results: æœ€å¤§è¿”å›æ•°
    Returns:
        NewsItemåˆ—è¡¨
    """
    try:
        if lang == 'zh':
            params = 'hl=zh-CN&gl=CN&ceid=CN:zh-Hans'
        else:
            params = 'hl=en-US&gl=US&ceid=US:en'

        # æ·»åŠ æ—¶é—´é™åˆ¶ï¼ˆwhen:1d = è¿‡å»24å°æ—¶ï¼‰
        q_encoded = quote(query + ' when:1d')
        url = f'https://news.google.com/rss/search?q={q_encoded}&{params}'

        resp = requests.get(url, headers=HEADERS, timeout=15)
        if resp.status_code != 200:
            return []

        feed = feedparser.parse(resp.text)
        items = []

        now_utc = datetime.now(timezone.utc)
        cutoff = now_utc - timedelta(hours=36)  # 36å°æ—¶çª—å£ï¼ˆå®½æ¾ä¸€ç‚¹ï¼‰

        for entry in feed.entries[:max_results * 2]:  # å¤šå–ä¸€äº›ç”¨äºè¿‡æ»¤
            pub_dt = _parse_pub_date(entry)

            # æ—¶æ•ˆæ€§è¿‡æ»¤ï¼šå¦‚æœèƒ½è§£ææ—¶é—´ï¼Œåªä¿ç•™36å°æ—¶å†…çš„
            if pub_dt and pub_dt < cutoff:
                continue

            raw_title = entry.get('title', '')
            title, source = _extract_source(raw_title)

            if not title or len(title) < 10:
                continue

            item = NewsItem(
                title=title,
                source=source,
                published=entry.get('published', ''),
                published_dt=pub_dt,
                url=entry.get('link', ''),
                language=lang,
            )
            items.append(item)

            if len(items) >= max_results:
                break

        return items

    except Exception as e:
        return []


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Skillçº§æ–°é—»é‡‡é›†ï¼ˆå¹¶è¡Œï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def fetch_skill_news(skill_id: int, max_per_query: int = 5) -> List[NewsItem]:
    """
    é‡‡é›†ç‰¹å®šSkillçš„æ–°é—»
    Args:
        skill_id: Skillç¼–å· (1-10)
        max_per_query: æ¯ä¸ªæŸ¥è¯¢æœ€å¤§è¿”å›æ•°
    Returns:
        å»é‡åçš„NewsItemåˆ—è¡¨
    """
    queries = SKILL_QUERIES.get(skill_id)
    if not queries:
        return []

    all_items = []
    seen_titles = set()

    # è‹±æ–‡æŸ¥è¯¢
    for q in queries.get('en', []):
        items = fetch_google_news(q, lang='en', max_results=max_per_query)
        for item in items:
            # æ ‡é¢˜å»é‡ï¼ˆç”¨æ ‡é¢˜å‰30å­—ç¬¦çš„hashï¼‰
            key = hashlib.md5(item.title[:30].lower().encode()).hexdigest()
            if key not in seen_titles:
                seen_titles.add(key)
                item.skill_tags.append(skill_id)
                all_items.append(item)

    # ä¸­æ–‡æŸ¥è¯¢
    for q in queries.get('zh', []):
        items = fetch_google_news(q, lang='zh', max_results=max_per_query)
        for item in items:
            key = hashlib.md5(item.title[:30].lower().encode()).hexdigest()
            if key not in seen_titles:
                seen_titles.add(key)
                item.skill_tags.append(skill_id)
                all_items.append(item)

    # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°ä¼˜å…ˆï¼‰
    all_items.sort(key=lambda x: x.published_dt or datetime.min.replace(tzinfo=timezone.utc), reverse=True)

    return all_items


def fetch_all_skills_news(max_per_skill: int = 8, parallel: bool = True) -> Dict[int, List[NewsItem]]:
    """
    å¹¶è¡Œé‡‡é›†æ‰€æœ‰10ä¸ªSkillçš„æ–°é—»
    Args:
        max_per_skill: æ¯ä¸ªSkillæœ€ç»ˆä¿ç•™çš„æœ€å¤§æ–°é—»æ•°
        parallel: æ˜¯å¦å¹¶è¡Œé‡‡é›†
    Returns:
        {skill_id: [NewsItem, ...]} å­—å…¸
    """
    print("  ğŸ“° é‡‡é›†è´¢ç»æ–°é—»ï¼ˆ10ä¸ªSkillé¢†åŸŸ Ã— åŒè¯­ï¼‰...")
    start = time.time()
    results = {}

    if parallel:
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = {}
            for skill_id in range(1, 11):
                future = executor.submit(fetch_skill_news, skill_id, max_per_skill)
                futures[future] = skill_id

            for future in as_completed(futures):
                skill_id = futures[future]
                try:
                    items = future.result(timeout=30)
                    results[skill_id] = items[:max_per_skill]
                except Exception:
                    results[skill_id] = []
    else:
        for skill_id in range(1, 11):
            items = fetch_skill_news(skill_id, max_per_skill)
            results[skill_id] = items[:max_per_skill]

    elapsed = time.time() - start
    total = sum(len(v) for v in results.values())
    skills_with_news = sum(1 for v in results.values() if v)
    print(f"  âœ… æ–°é—»é‡‡é›†å®Œæˆ: {total}æ¡æ–°é—» è¦†ç›–{skills_with_news}/10ä¸ªSkill ({elapsed:.1f}ç§’)")

    # æ‰“å°å„Skillé‡‡é›†æ¦‚å†µ
    for sid in range(1, 11):
        items = results.get(sid, [])
        name = SKILL_QUERIES[sid]['name']
        if items:
            print(f"    Skill {sid:>2} {name}: {len(items)}æ¡ | {items[0].title[:50]}...")
        else:
            print(f"    Skill {sid:>2} {name}: 0æ¡")

    return results


def format_news_for_skill(news_items: List[NewsItem], max_display: int = 5) -> str:
    """å°†æ–°é—»åˆ—è¡¨æ ¼å¼åŒ–ä¸ºSkillåˆ†æä½¿ç”¨çš„æ‘˜è¦æ–‡æœ¬"""
    if not news_items:
        return ""

    lines = []
    for item in news_items[:max_display]:
        source_tag = f"[{item.source}]" if item.source != 'Unknown' else ""
        lang_tag = "ğŸ‡¨ğŸ‡³" if item.language == 'zh' else "ğŸ‡ºğŸ‡¸"
        lines.append(f"{lang_tag} {source_tag} {item.title}")

    return "\n".join(lines)


def format_news_for_markdown(news_items: List[NewsItem], max_display: int = 5) -> List[str]:
    """å°†æ–°é—»åˆ—è¡¨æ ¼å¼åŒ–ä¸ºMarkdownè¡¨æ ¼è¡Œ"""
    rows = []
    for item in news_items[:max_display]:
        lang_tag = "ğŸ‡¨ğŸ‡³" if item.language == 'zh' else "ğŸ‡ºğŸ‡¸"
        source = item.source[:15] if item.source else '-'
        title = item.title[:60] + ('...' if len(item.title) > 60 else '')
        pub = item.published[:16] if item.published else '-'
        rows.append(f"| {lang_tag} | {source} | {title} | {pub} |")
    return rows


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ–°é—»ç¼“å­˜ï¼ˆé¿å…é‡å¤è¯·æ±‚ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def save_news_cache(all_news: Dict[int, List[NewsItem]]):
    """å°†æ–°é—»ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶"""
    os.makedirs(CACHE_DIR, exist_ok=True)
    cache_file = os.path.join(CACHE_DIR, f"news_{datetime.now().strftime('%Y%m%d_%H%M')}.json")

    cache_data = {}
    for skill_id, items in all_news.items():
        cache_data[str(skill_id)] = [item.to_dict() for item in items]

    with open(cache_file, 'w', encoding='utf-8') as f:
        json.dump(cache_data, f, ensure_ascii=False, indent=2)

    return cache_file


def load_latest_cache() -> Optional[Dict[int, List[NewsItem]]]:
    """åŠ è½½æœ€è¿‘6å°æ—¶å†…çš„ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
    if not os.path.exists(CACHE_DIR):
        return None

    files = sorted([f for f in os.listdir(CACHE_DIR) if f.startswith('news_') and f.endswith('.json')], reverse=True)
    if not files:
        return None

    latest = files[0]
    # æ£€æŸ¥æ—¶æ•ˆæ€§ï¼ˆ6å°æ—¶ä»¥å†…æœ‰æ•ˆï¼‰
    try:
        ts_str = latest.replace('news_', '').replace('.json', '')
        cache_time = datetime.strptime(ts_str, '%Y%m%d_%H%M')
        if datetime.now() - cache_time > timedelta(hours=6):
            return None
    except Exception:
        return None

    try:
        with open(os.path.join(CACHE_DIR, latest), 'r', encoding='utf-8') as f:
            data = json.load(f)

        results = {}
        for skill_id_str, items in data.items():
            skill_id = int(skill_id_str)
            results[skill_id] = [
                NewsItem(
                    title=item['title'],
                    source=item['source'],
                    published=item['published'],
                    url=item.get('url', ''),
                    language=item.get('language', 'en'),
                    skill_tags=item.get('skill_tags', []),
                )
                for item in items
            ]
        return results
    except Exception:
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»å…¥å£
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_all_news(force_refresh: bool = False) -> Dict[int, List[NewsItem]]:
    """
    è·å–æ‰€æœ‰Skillçš„æ–°é—»ï¼ˆå¸¦ç¼“å­˜ï¼‰
    Args:
        force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
    Returns:
        {skill_id: [NewsItem, ...]}
    """
    if not force_refresh:
        cached = load_latest_cache()
        if cached:
            total = sum(len(v) for v in cached.values())
            print(f"  ğŸ“° ä½¿ç”¨ç¼“å­˜æ–°é—»: {total}æ¡")
            return cached

    all_news = fetch_all_skills_news(max_per_skill=8, parallel=True)
    save_news_cache(all_news)
    return all_news


# æµ‹è¯•
if __name__ == "__main__":
    news = get_all_news(force_refresh=True)
    print(f"\n{'='*60}")
    for sid in range(1, 11):
        items = news.get(sid, [])
        print(f"\n--- Skill {sid}: {SKILL_QUERIES[sid]['name']} ({len(items)}æ¡) ---")
        for item in items[:3]:
            lang = 'ğŸ‡¨ğŸ‡³' if item.language == 'zh' else 'ğŸ‡ºğŸ‡¸'
            print(f"  {lang} [{item.source}] {item.title[:70]}")
